# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgxsN2Ff1uajyg395ERedKtbKaXjiDGg
"""

import pandas as pd

# Load the CSV file
df = pd.read_csv('/content/sample_data/amazon.csv')

"""Clean Price Columns (discounted_price, actual_price)"""

def clean_price(price_str):
    if isinstance(price_str, str):
        return float(price_str.replace('‚Çπ', '').replace(',', '').strip())
    return None

df['discounted_price'] = df['discounted_price'].apply(clean_price)
df['actual_price'] = df['actual_price'].apply(clean_price)

"""Convert Discount Percentage"""

df['discount_percentage'] = df['discount_percentage'].str.replace('%', '').astype(float)

df['rating'] = pd.to_numeric(df['rating'], errors='coerce')

df['rating_count'] = df['rating_count'].str.replace(',', '')
df['rating_count'] = pd.to_numeric(df['rating_count'], errors='coerce')

category_split = df['category'].str.split('|', expand=True)
df['category_level_1'] = category_split[0]
df['category_level_2'] = category_split[1]
df['category_level_3'] = category_split[2]

print(df.info())
print(df[['discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count']].describe())

"""EDA

Top-Selling Products
"""

top_products = df.sort_values(by='rating_count', ascending=False).head(10)

"""Effect of Discount on Sales"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(data=df, x='discount_percentage', y='rating_count')
plt.title('Discount % vs. Rating Count')
plt.show()

"""Rating Distribution"""

sns.histplot(df['rating'], bins=10, kde=True)

"""Average Price by Category"""

df.groupby('category_level_2')['discounted_price'].mean().sort_values(ascending=False).plot(kind='barh')

"""Best Reviewd Categories"""

df.groupby('category_level_1')['rating'].mean().sort_values(ascending=False).plot(kind='bar')

"""PRECITIVE INSIGHTS

Regression: Predict Rating Count (proxy for popularity)
"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Drop rows with missing values in selected columns
df_model = df[['discounted_price', 'discount_percentage', 'rating', 'rating_count']].dropna()

X = df_model[['discounted_price', 'discount_percentage', 'rating']]
y = df_model['rating_count']

# üîÄ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# üß† Train Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# üìà Evaluate
y_pred = rf_model.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"‚úÖ Random Forest R¬≤ Score: {r2:.3f}")
print(f"üìâ RMSE: {rmse:.2f}")

# üîç Feature Importance Plot
importances = rf_model.feature_importances_
feature_names = X.columns

plt.figure(figsize=(8, 5))
plt.barh(feature_names, importances)
plt.xlabel("Feature Importance")
plt.title("Feature Importance in Predicting Rating Count")
plt.grid(True)
plt.tight_layout()
plt.show()

"""The RMSE is so high because of highky skewed rating_count. To better tackle the problem, I will use randomforestclassifier"""

# üì¶ Imports

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix


# üè∑Ô∏è Define popularity classes
quantiles = df_model['rating_count'].quantile([0.33, 0.66])
def categorize_popularity(x):
    if x <= quantiles[0.33]:
        return 'Low'
    elif x <= quantiles[0.66]:
        return 'Medium'
    else:
        return 'High'

df_model['popularity_class'] = df_model['rating_count'].apply(categorize_popularity)

# üéØ Features and Target
X = df_model[['discounted_price', 'discount_percentage', 'rating']]
y = df_model['popularity_class']

# üîÄ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# üß† Train Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# üìä Predict and Evaluate
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

# üìâ Confusion Matrix Plot
cm = confusion_matrix(y_test, y_pred, labels=['Low', 'Medium', 'High'])
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Low', 'Medium', 'High'],
            yticklabels=['Low', 'Medium', 'High'])

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Popularity Classifier')
plt.tight_layout()
plt.show()

"""High and Low classes are predicted reasonably well, but when it comes to Medium class is often confused with both Low and High, suggesting overlap in feature values.

To improve the accuracy, I'll add more features in the feature engineering
"""

# üß† Feature Engineering
df['price_diff'] = df['actual_price'] - df['discounted_price']
df['is_heavy_discount'] = (df['discount_percentage'] > 50).astype(int)
df['product_name_length'] = df['product_name'].apply(lambda x: len(str(x)))
df['about_word_count'] = df['about_product'].apply(lambda x: len(str(x).split()))

# Select features & target
feature_columns = ['discounted_price', 'discount_percentage', 'rating',
                   'price_diff', 'is_heavy_discount', 'product_name_length', 'about_word_count']
df_model = df[feature_columns + ['rating_count']].dropna()

# üéØ Create Popularity Classes
quantiles = df_model['rating_count'].quantile([0.33, 0.66])
def categorize_popularity(x):
    if x <= quantiles[0.33]:
        return 'Low'
    elif x <= quantiles[0.66]:
        return 'Medium'
    else:
        return 'High'

df_model['popularity_class'] = df_model['rating_count'].apply(categorize_popularity)

# üß™ Train/Test Split
X = df_model[feature_columns]
y = df_model['popularity_class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# üß† Train Random Forest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# üìà Predict and Evaluate
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

# üìä Confusion Matrix Plot
cm = confusion_matrix(y_test, y_pred, labels=['Low', 'Medium', 'High'])
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Low', 'Medium', 'High'],
            yticklabels=['Low', 'Medium', 'High'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Popularity Classifier')
plt.tight_layout()
plt.show()